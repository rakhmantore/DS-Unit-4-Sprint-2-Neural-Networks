{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_Train_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Train Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
        "\n",
        "*Don't forgot to switch to GPU on Colab!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJ2b3wk62Ud"
      },
      "source": [
        "### Write a function to load your data\n",
        "\n",
        "Wrap yesterday's preprocessing steps into a function that returns four items:\n",
        "* X_train\n",
        "* y_train\n",
        "* X_test\n",
        "* y_test\n",
        "\n",
        "Your function should accept a `path` to the data as a argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLhmrViLs8Wn"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJsIsrvp7O3e"
      },
      "source": [
        "def load_quickdraw10(path):\n",
        "  data = np.load(path)\n",
        "  X = data['arr_0']\n",
        "  y = data['arr_1']\n",
        "  X, y = shuffle(X, y)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
        "  X_train = X_train / 255.\n",
        "  X_test = X_test / 255.\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-LjQLBJvis4"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_quickdraw10('quickdraw10.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Y7GDJTvuW4",
        "outputId": "9ff35fe3-19fc-4dbf-8754-d0226f26aa4b"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-6PxI6H5__2"
      },
      "source": [
        "### Write a Model Function\n",
        "Using your model from yesterday, write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. Include parameters for the following: \n",
        "* Learning Rate\n",
        "* Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NfcNLjgwVZH"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEREYT-3wI1f"
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "def create_model(lr=0.01, opt = 'Adam'):\n",
        "  if opt == 'Adam':\n",
        "    opt = Adam(learning_rate=lr)\n",
        "  elif opt == 'Adagrad':\n",
        "    opt = Adagrad(learning_rate=lr)\n",
        "  elif opt == 'Adadelta':\n",
        "    opt = Adadelta(learning_rate=lr)\n",
        "  elif opt == 'SGD':\n",
        "    opt = SGD(learning_rate=lr)\n",
        "  elif opt == 'Nadam':\n",
        "    opt = Nadam(learning_rate=lr)\n",
        "  model = Sequential([\n",
        "    Dense(32, activation='sigmoid', input_dim=784),\n",
        "    Dense(32, activation='sigmoid'),\n",
        "    Dense(10, activation='softmax')\n",
        "    ])\n",
        "  model.compile(optimizer=opt, loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-BOvI4XKAvg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0pCkh8C7eGL"
      },
      "source": [
        "### Experiment with Batch Size\n",
        "* Run 5 experiments with various batch sizes of your choice. \n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "### Experiment with Learning Rate\n",
        "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
        "* Use the \"best\" batch size from the previous experiment\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SA144xx8Luf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxMtSRhV9Q7I"
      },
      "source": [
        "### Experiment with different Optimizers\n",
        "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
        "* Repeat the experiment combining Learning Rate and different optimizers. Does the best performing model change? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujLuzdNA91ip"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydAqeY9S8uHA"
      },
      "source": [
        "### Additional Written Tasks\n",
        "\n",
        "1. Describe the process of backpropagation in your own words: \n",
        "```\n",
        "Your answer goes here.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Implement GridSearch on anyone of the experiments\n",
        "- On the learning rate experiments, implement [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n",
        "- Review material on the math behind gradient descent: \n",
        "\n",
        "  - Gradient Descent\n",
        "    - Gradient Descent, Step-by-Step  by StatQuest w/ Josh Starmer. This will help you understand the gradient descent based optimization that happens underneath the hood of neural networks. It uses a non-neural network example, which I believe is a gentler introduction. You will hear me refer to this technique as \"vanilla\" gradient descent. \n",
        "    - Stochastic Gradient Descent, Clearly Explained!!! by StatQuest w/ Josh Starmer. This builds on the techniques in the previous video.  This technique is the one that is actually implemented inside modern 'nets. \n",
        "These are great resources to help you understand tomorrow's material at a deeper level. I highly recommend watching these ahead of tomorrow.\n",
        "\n",
        "  - Background Math\n",
        "    - Dot products and duality by 3Blue1Brown. Explains the core linear algebra operation happening in today's perceptron.\n",
        "The paradox of the derivative by 3Blue1Brown. Does a great job explaining a derivative. \n",
        "    - Visualizing the chain rule and product rule by 3Blue1Brown. Explains the black magic that happens within Stochastic Gradient Descent. \n",
        "These math resources are very much optional. They can be very heady, but I encourage you to explore. Your understanding of neural networks will greatly increase if you understand this math background.\n",
        "\n",
        "\n"
      ]
    }
  ]
}